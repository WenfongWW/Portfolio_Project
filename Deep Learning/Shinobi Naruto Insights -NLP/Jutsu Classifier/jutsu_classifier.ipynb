{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'maybe_sync' from 'fsspec.asyn' (c:\\Users\\Wen\\anaconda3\\Lib\\site-packages\\fsspec\\asyn.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclass_weight\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_class_weight\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForSequenceClassification\n",
      "File \u001b[1;32mc:\\Users\\Wen\\anaconda3\\Lib\\site-packages\\datasets\\__init__.py:43\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m pyarrow\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m version\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[1;32mc:\\Users\\Wen\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:66\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowReader\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_download_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m xgetsize\n",
      "File \u001b[1;32mc:\\Users\\Wen\\anaconda3\\Lib\\site-packages\\datasets\\arrow_writer.py:27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpq\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Features, Image, Value\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     FeatureType,\n\u001b[0;32m     30\u001b[0m     _ArrayXDExtensionType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     to_pyarrow_listarray,\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilesystems\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_remote_filesystem\n",
      "File \u001b[1;32mc:\\Users\\Wen\\anaconda3\\Lib\\site-packages\\datasets\\features\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[0;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArray2D\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslationVariableLanguages\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m ]\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Array2D, Array3D, Array4D, Array5D, ClassLabel, Features, Sequence, Value\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "File \u001b[1;32mc:\\Users\\Wen\\anaconda3\\Lib\\site-packages\\datasets\\features\\audio.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_download_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m xopen, xsplitext\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_cast\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpy_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m no_op_if_value_is_null, string_to_dict\n",
      "File \u001b[1;32mc:\\Users\\Wen\\anaconda3\\Lib\\site-packages\\datasets\\download\\__init__.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadManager, DownloadMode\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_download_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StreamingDownloadManager\n",
      "File \u001b[1;32mc:\\Users\\Wen\\anaconda3\\Lib\\site-packages\\datasets\\download\\streaming_download_manager.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maiohttp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClientError\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilesystems\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COMPRESSION_FILESYSTEMS\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     get_authentication_headers_for_url,\n\u001b[0;32m     24\u001b[0m     http_head,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     url_or_path_join,\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_logger\n",
      "File \u001b[1;32mc:\\Users\\Wen\\anaconda3\\Lib\\site-packages\\datasets\\filesystems\\__init__.py:16\u001b[0m\n\u001b[0;32m     13\u001b[0m _has_s3fs \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mfind_spec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3fs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_s3fs:\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ms3filesystem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m S3FileSystem  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     18\u001b[0m COMPRESSION_FILESYSTEMS: List[compression\u001b[38;5;241m.\u001b[39mBaseCompressedFileFileSystem] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     19\u001b[0m     compression\u001b[38;5;241m.\u001b[39mBz2FileSystem,\n\u001b[0;32m     20\u001b[0m     compression\u001b[38;5;241m.\u001b[39mGzipFileSystem,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     compression\u001b[38;5;241m.\u001b[39mZstdFileSystem,\n\u001b[0;32m     24\u001b[0m ]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Register custom filesystems\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Wen\\anaconda3\\Lib\\site-packages\\datasets\\filesystems\\s3filesystem.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01ms3fs\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mS3FileSystem\u001b[39;00m(s3fs\u001b[38;5;241m.\u001b[39mS3FileSystem):\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    `datasets.filesystems.S3FileSystem` is a subclass of [`s3fs.S3FileSystem`](https://s3fs.readthedocs.io/en/latest/api.html).\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Wen\\anaconda3\\Lib\\site-packages\\s3fs\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m S3FileSystem, S3File\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m S3Map\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_versions\n",
      "File \u001b[1;32mc:\\Users\\Wen\\anaconda3\\Lib\\site-packages\\s3fs\\core.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AbstractBufferedFile\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_storage_options, tokenize\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masyn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncFileSystem, sync, sync_wrapper, maybe_sync\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maiobotocore\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'maybe_sync' from 'fsspec.asyn' (c:\\Users\\Wen\\anaconda3\\Lib\\site-packages\\fsspec\\asyn.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data prepration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jutsu_name</th>\n",
       "      <th>jutsu_type</th>\n",
       "      <th>jutsu_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Hit Combo</td>\n",
       "      <td>Taijutsu</td>\n",
       "      <td>Lars punches the opponent before striking them...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avalanche Dance</td>\n",
       "      <td>Taijutsu, Shurikenjutsu</td>\n",
       "      <td>Haku delivers a string of kicks, punches, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attack of the Twin Demons</td>\n",
       "      <td>Kekkei Genkai, Ninjutsu</td>\n",
       "      <td>This technique allows Ukon to inhabit his brot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bando's Chakra Armour</td>\n",
       "      <td>Ninjutsu</td>\n",
       "      <td>Bando activates his chakra to surround him in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ball of Light Technique</td>\n",
       "      <td>Ninjutsu</td>\n",
       "      <td>The user gathers a ball of concentrated light ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  jutsu_name               jutsu_type  \\\n",
       "0               10 Hit Combo                 Taijutsu   \n",
       "1            Avalanche Dance  Taijutsu, Shurikenjutsu   \n",
       "2  Attack of the Twin Demons  Kekkei Genkai, Ninjutsu   \n",
       "3      Bando's Chakra Armour                 Ninjutsu   \n",
       "4    Ball of Light Technique                 Ninjutsu   \n",
       "\n",
       "                                   jutsu_description  \n",
       "0  Lars punches the opponent before striking them...  \n",
       "1  Haku delivers a string of kicks, punches, and ...  \n",
       "2  This technique allows Ukon to inhabit his brot...  \n",
       "3  Bando activates his chakra to surround him in ...  \n",
       "4  The user gathers a ball of concentrated light ...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"C:/Users/Wen/OneDrive/Documents/GitHub/Portfolio_Project/Deep Learning/Shinobi Naruto Insights -NLP/Web Scraping/jutsu.jsonl\"\n",
    "\n",
    "df = pd.read_json(data_path, lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3_types_of_jutsu\n",
       "Ninjutsu    2009\n",
       "Taijutsu     628\n",
       "Genjutsu      79\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jutsu_types(jutsu):\n",
    "    if \"Taijutsu\" in jutsu:\n",
    "        return \"Taijutsu\"\n",
    "    if \"Ninjutsu\" in jutsu:\n",
    "        return \"Ninjutsu\"\n",
    "    if \"Genjutsu\" in jutsu:\n",
    "        return \"Genjutsu\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "df[\"3_types_of_jutsu\"] = df[\"jutsu_type\"].apply(jutsu_types)\n",
    "df[\"3_types_of_jutsu\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['jutsu_name', 'jutsu_type', 'jutsu_description', '3_types_of_jutsu'], dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text     jutsu\n",
      "0     10 Hit Combo. Lars punches the opponent before...  Taijutsu\n",
      "1     Avalanche Dance. Haku delivers a string of kic...  Taijutsu\n",
      "2     Attack of the Twin Demons. This technique allo...  Ninjutsu\n",
      "3     Bando's Chakra Armour. Bando activates his cha...  Ninjutsu\n",
      "4     Ball of Light Technique. The user gathers a ba...  Ninjutsu\n",
      "...                                                 ...       ...\n",
      "2867  Absolute: Fang Passing Fang. Kiba and Akamaru ...  Taijutsu\n",
      "2868  16 Hit Combo. A very effective move, Ino uses ...  Taijutsu\n",
      "2869  1000 Metre Punch. The user focuses a large amo...  Taijutsu\n",
      "2870  100% Single Punch. Tsunade gathers large amoun...  Taijutsu\n",
      "2871  100 Metre Punch. A shorter version of the 1000...  Taijutsu\n",
      "\n",
      "[2716 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['jutsu_name']+ '. '+df['jutsu_description']\n",
    "df['jutsu'] = df['3_types_of_jutsu']\n",
    "df= df[['text','jutsu']].dropna()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Clean Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_line_breaks(self, text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cleaner():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def remove_html_tags(self, text):\n",
    "        \"\"\"Remove HTML tags using BeautifulSoup.\"\"\"\n",
    "        cleantext = BeautifulSoup(text, \"lxml\").text\n",
    "        return cleantext\n",
    "    \n",
    "    def clean(self, text):\n",
    "        \"\"\"Clean text by removing HTML tags and standardizing white space.\"\"\"\n",
    "        text = self.remove_html_tags(text)\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wen\\AppData\\Local\\Temp\\ipykernel_5512\\3021304313.py:7: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  cleantext = BeautifulSoup(text, \"lxml\").text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       10 Hit Combo. Lars punches the opponent before...\n",
      "1       Avalanche Dance. Haku delivers a string of kic...\n",
      "2       Attack of the Twin Demons. This technique allo...\n",
      "3       Bando's Chakra Armour. Bando activates his cha...\n",
      "4       Ball of Light Technique. The user gathers a ba...\n",
      "                              ...                        \n",
      "2867    Absolute: Fang Passing Fang. Kiba and Akamaru ...\n",
      "2868    16 Hit Combo. A very effective move, Ino uses ...\n",
      "2869    1000 Metre Punch. The user focuses a large amo...\n",
      "2870    100% Single Punch. Tsunade gathers large amoun...\n",
      "2871    100 Metre Punch. A shorter version of the 1000...\n",
      "Name: text_cleaned, Length: 2716, dtype: object\n"
     ]
    }
   ],
   "source": [
    "cleaned_data = Cleaner()\n",
    "df['text_cleaned'] = df[\"text\"].apply(cleaned_data.clean)\n",
    "print(df['text_cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>jutsu</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Hit Combo. Lars punches the opponent before...</td>\n",
       "      <td>Taijutsu</td>\n",
       "      <td>10 Hit Combo. Lars punches the opponent before...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avalanche Dance. Haku delivers a string of kic...</td>\n",
       "      <td>Taijutsu</td>\n",
       "      <td>Avalanche Dance. Haku delivers a string of kic...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attack of the Twin Demons. This technique allo...</td>\n",
       "      <td>Ninjutsu</td>\n",
       "      <td>Attack of the Twin Demons. This technique allo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bando's Chakra Armour. Bando activates his cha...</td>\n",
       "      <td>Ninjutsu</td>\n",
       "      <td>Bando's Chakra Armour. Bando activates his cha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ball of Light Technique. The user gathers a ba...</td>\n",
       "      <td>Ninjutsu</td>\n",
       "      <td>Ball of Light Technique. The user gathers a ba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     jutsu  \\\n",
       "0  10 Hit Combo. Lars punches the opponent before...  Taijutsu   \n",
       "1  Avalanche Dance. Haku delivers a string of kic...  Taijutsu   \n",
       "2  Attack of the Twin Demons. This technique allo...  Ninjutsu   \n",
       "3  Bando's Chakra Armour. Bando activates his cha...  Ninjutsu   \n",
       "4  Ball of Light Technique. The user gathers a ba...  Ninjutsu   \n",
       "\n",
       "                                        text_cleaned  label  \n",
       "0  10 Hit Combo. Lars punches the opponent before...      2  \n",
       "1  Avalanche Dance. Haku delivers a string of kic...      2  \n",
       "2  Attack of the Twin Demons. This technique allo...      1  \n",
       "3  Bando's Chakra Armour. Bando activates his cha...      1  \n",
       "4  Ball of Light Technique. The user gathers a ba...      1  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(df[\"jutsu\"].tolist())\n",
    "df['label'] = encoder.transform(df[\"jutsu\"].tolist())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.459915611814345, 0.45063879210220675, 1.4416135881104033]\n"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weight('balanced',\n",
    "                     classes=sorted(df['label'].unique().tolist()),\n",
    "                     y=df['label'].tolist()).tolist()\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Train Testing Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_test = train_test_split(df,test_size=0.2,stratify=df['label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Convert to HuggingFace Datast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(df_train)\n\u001b[0;32m      2\u001b[0m test_data \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(df_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_data = Dataset.from_pandas(df_train)\n",
    "test_data = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text_cleaned\"], truncation=True)\n",
    "\n",
    "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_data.map(preprocess_function, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weights).to(device=device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    logging_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('jutsu_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(tokenized_train)\n",
    "preds = np.argmax(preds[:3][0],axis=1)\n",
    "actual = df_train['label'].tolist()\n",
    "print(classification_report(actual,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(tokenized_test)\n",
    "preds = np.argmax(preds[:3][0],axis=1)\n",
    "actual = df_test['label'].tolist()\n",
    "print(classification_report(actual,preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
